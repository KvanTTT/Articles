# Теория и практика формальных языков

Привет всем! Меня зовут Иван Кочуркин. Я работаю в Positive Technologies
над универсальными анализаторами исходных кодов и подрабатыватю в Swiftify,
веб-сервисе для преобразования кода Objective-C в Swift. Также я веду
активную деятельность на GitHub и пишу статьи на хабре на техническую тематику.

Вся моя работа и хобби так или иначе связаны с парсингом как небольших предметно-
ориентированных языков (т.е. DSL), так и полноценные языков программирования
и декларативных языков.

Например, в Positive Technologies я занимался и занимаюсь разработкой языка описания шаблонов,
предназначеных для описания различных недостатков и уязвимостей в исходном коде.
Также я занимался разработкой парсеров для языков Java, PHP, PL/SQL, T-SQL, Objective-C.

В этом докладе я хотел бы рассказать о базовых понятиях парсинга, о том, какие
библиотеки парсинга существуют под платформу .NET и язык C#. А потом я хотел бы
углубиться в проблемы парсинга на примере и сравнении таких инструментов как
ANTLR и Roslyn. В завершающей части доклада я коснусь темы обработки результатов
парсинга, т.е. деревьев.

Сразу предупрежу, что доклад не совсем связан с .NET, однако практически
весь материал носит прикладное значение и используется как в открытых, так и в
коммерческих проектах. Более того, большую часть материала можно найти на гитхабе!

# Почему не Regex?

Итак, начнем со всеми известными и любимыми регулярными выражениями? Почему их
не достаточно для парсинга всего? Я думаю большая часть аудитории так или иначе
догадываются. И, несмотря на то, что современные регулярки, например в C#, обладают
мощностьюконтекстно-свободных языков, описывать ими конструкции с определенного этапа
становится очень сложно.

Рассмотрим такую регулярку: `<table>(.*?)</table>`. Данное выражение
парсит не жадно текст до закрывающегося тега `</table>`. Ок, все хорошо, все довольно.
А что, если нужно добавить еще и обработку аттрибутов? Ок, нет проблем,
используем такую: `<table.*?>(.*?)</table>`. А что если дальше нужно добавить
поддержку элементов `<tr>` и `<td>`, которые внутри себя также могут содержать
кучу тегов? Или, допустим, добавить поддержку комментариев `<!-- html comment -->`.
После определенного этапа вам не захочется вносить изменения в эту сложное
регулярное выражение и вы поймете, что здесь что-то не так :)

Кстати, то же самое касается и парсинга любых языков, в котором есть вложенные
элементы, например простейшие блоки из фигурных скобок в C#.

# Лексемы и токены

Ну ок, попробуем придумать способ как отфильтровать хотя бы комментарии в html.
<!-- Спросить у зала--> Как известно, комментарии начинаются и заканчиваются
определенными маркерами или символами последовательность. Так значит
можно заранее детектировать текст заключенный в такие маркеры, удалять его или
помещать куда-то в другое место и парсить дальше как ни в чем не бывало!
Круто, но получается обработка в два прохода. А что, если сделать это в один
проход и сразу создавать структурки для таких вот последовательностей символов,
при этом назначая на этом проходе каждой последовательности тип! А помимо
комментариев так же можно определять пробелы, числа, идентификаторы, строки и другие
типы.

Тоже вполне себе подход. Более того, для таких последовательностей даже
существуют общепринятые определения. **Токеном** обозначается последовательность
определенных символов и их тип. Саму эту последовательность обычно называют
**лексемой**. <!-- Более того, сами типы токенов можно использовать в конечном автомате. -->
Процесс создания токенов из текста называется **токенизацией**.

Для описания токенизаторов существуют свои языки и инструменты. Обычно каждый
токен описывается на новой строке, слева прописан идентификатор токена, а справа -
его значение или лексема. Вот простейший пример:

```ANTLR
MyKeyword: `var`;
Id:        [a-z]+;
Digit:     [0-9]+;
```

# Дерево разбора и AST

Однако наша проблема с regex решена не полностью! Со скрытыми символами разобрались,
но как обрабатывать вложенные теги? Ведь токенизатор возвращает нам поток токенов,
а у нас на лицо рекурсивная структура!

<!-- TODO: возможно добавить фрагмент кода с рекурсией -->

Или как распарсить структуру, если ее дальнейший парсинг зависит от первого токена?
Так вот, таких случаев и были разработаны разные алгоритмы и практики
парсинга. Если поток токенов представляет собой линейную последовательность, то
распознанные структуры уже более сложные: они представляют собой деревья, а именно
иерархический список узлов, каждый из которых содержит в себе дочерние узлы или дети.
Такие структуры называются **деревьями разбора** или **конкретными деревьями синтаксиса** (**CST**).

Некоторые элементы в языке в принципе нужны только для корректного парсинга
и никак не влияют на код, который выдает компилятор языка, например, точка с
запятой в C#. Без комментариев и пробелов, понятное дело, тоже можно обойтись.
Так вот, *дерево разбора*, которое не содержит все такие не значимые токены,
называют великим и ужасным **абстрактным синтаксическим деревом** или **AST**.

Процесс создания деревьев разбора из потока токенов называется парсингом.
Стоит отметить, что парсеры бывают как классическими, принимающими на вход
поток токенов, так и **безлексерными**, принимающие на вход сырой исходник.

# Типы парсеров

Парсеры бывают нескольких типов:

* Готовые библиотеки парсинга
* Парсеры, написанные вручную
* И автоматически сгенерированные парсеры из грамматик

Готовыми либами парсинга являются, например, встроенные движок регулярных выражений,
парсер XML и внешние библиотеки, например хорошо известный JSON.NET, ServiceStack и другие.
Их достоинствами являются хорошее качество, производительность, т.к. они
написаны для распространенных форматов, опенсорсные и существуют продолжительное
время. Также они имеют обширный API, позволяющий, например, прописывать
произвольную логику десериализации для каких-то сложных древовидных или графовых структур.
Однако такие парсеры не получится использовать для менее распространенных языков,
а тем более своих предметно-ориентированных или DSL (Domain Specific Language).

Вторым типом являются парсеры, написанные полностью вручную. Такие парсеры
наиболее гибкие в возможностях и имеют максимальную производительность.
Однако требуется большое погружение в предметную область, даже большее, чем для
генераторов парсеров. Разработка таких парсеров очень долгая и занимает
много-много человекочасов. Примером таких парсеров под C# являются Roslyn.
Roslyn представляет собой не просто парсер, а полноценным инструмент для парсинга,
анализа и компиляции C# и VB кода. Однако даже такой мощный фреймворк заточен
только на два языка, он не сможет парсить любые нужные нам языки, да и работает он
только под .NET.

Поэтому на помощь приходят такие инструменты, как **генераторы парсеров** или
**компиляторы компиляторов**. С помощью них можно генерировать парсеры сразу
под несколько языков (или рантаймов) имея только одно описание языка, которое
называет **грамматикой**. Также существуют библиотеки для создания парсеров в
рантайме для конкретного языка, а не на этапе генерации.
Такие инструменты называются **комбинаторами парсеров**.

# Грамматика

В грамматике описываются набор правил, каждое из которых описывает определенный
узел в дерева разбора. На этом слайде приведен простейши пример: в нем описано
правило для выражений сложения и умножения. Такое правило будет парсить текст
`a + b * c`.

```ANTLR
expr
    : expr '*' expr
    | expr '+' expr
    | ID '(' args ')'
    | ID
    ;

ID: [a-zA-Z]+;
```

Стоит отметить, что правило `expr` здесь ссылается само на себя в двух
альтернативах. Такие правила называются **лево-рекурсивными** и не все генераторы
парсеров способы справляться с такими правилами, поэтому приходится переписывать
их определенным образом.

---

# Типы языков

Вообще говоря, существует система классификации языков, которая называется
**иерархией Хомского**. Упрощенно она выглядит так и наши любимые регулярные
выражения по ней обладают наименьшей вычислительной мощностью, а практически
весь синтаксис современных языков программирования можно описать с помощью
контекстно-зависимых грамматик.

* Регулярные
* Контекстно-свободные
* Контекстно-зависимые
* Тьюринг-полные

Если сравнивать парсеры КС-языков и регулярных выражений «на пальцах»,
то последние не имеют памяти. А если сравнивать парсеры КЗ- и КС-языков,
то последние не запоминают значения (не типы) посещенных ранее токенов.

Кроме того, язык в одном случае может являться КС, а в другом — КЗ.
Если учитывать **семантику**, т.е. согласованность с определениями языка,
в частности, согласованность типов, то язык может рассматриваться как КЗ.

---

# Инструменты и библиотеки под C#

Под C# существует обширное количество библиотек для парсинга, охватывающие
как регулярные, так и контекстно-свободные языки, с поддержкой левой рекурсиии
и без, использующие лексер и нет, а также предоставляющие средства анализа кода,
а не только парсинга.

Nitra, JetBrains MPS представляют собой не просто пасреры, а целый языковые
фреймворки (их еще называют Language Workbench). Они предназначены для создания,
расширения языков, описания семантики, кодогенерации, интеграции языка с IDE.
В общем представляют собой эдакое решение "все из коробки". Ну и Roslyn
туда же.

Подробно я об остальных инструментах рассказывать не буду, разве что коснусь
комбинаторов. Однако все инструменты и библиотеки детально расписаны в недавней статье
[Parsing In C#: Tools And Libraries](https://tomassetti.me/parsing-in-csharp).

# Парсер-комбинаторы

Как я уже упоминал, комбинаторы парсеров встроены в язык, а значит никакая
генерация не нужна. В некоторых случаях это немного упрощает рабочий процесс, т.к.
отпадает необходимость настраивания его в CI. Также такие комбинаторы могут быть
интегрированы с IDE разработчика. На практике комбинаторы очень полезны для
мелких задачек парсинга, чуть сложнее чем регулярные выражения. В общем если вы
нуждаетесь в парсере, но не хотите сильно вникать в предметную область, то
комбинаторы парсеров - ваш выбор.

# Примеры кода парсер-комбинатора

Вот пример простейшего кода на библиотеке парсинга [Sprache](https://github.com/sprache/Sprache):

```CSharp
// Parse any number of capital 'A's in a row
var parseA = Parse.Char('A').AtLeastOnce();
```

Или, например, правило для идентификатора. В нем мы видим, что лидирующие и
замыкающие пробелы игнорируются, а сам идентификатор должен начинаться с буквы.

```CSharp
Parser<string> identifier =
    from leading in Parse.WhiteSpace.Many()
    from first in Parse.Letter.Once()
    from rest in Parse.LetterOrDigit.Many()
    from trailing in Parse.WhiteSpace.Many()
    select new string(first.Concat(rest).ToArray());
    
var id = identifier.Parse(" abc123  ");

Assert.AreEqual("abc123", id);
```

# Проблемы и задачи парсинга

Далее я опишу несколько проблем и задач, с которыми мне пришлось столкнуться
на практике при использовании генератора парсера ANTLR и полноценного
фреймворка Roslyn.

Несмотря на то, что это совсем различные инструменты: Roslyn полностью написан
вручную, парсит и анализирует C# и VB. Но по некоторым критериям они схожи,
и я попытался сравнить их.

В конце заголовка у некоторых задач в скобочках стоит название PT.PM, Roslyn,
Swiftify. Оно означает проект, в котором данная задача появилась и была решена.
PT.PM подразумевает грамматики PL/SQL, T-SQL, PHP, Java 8, а также
экспериментальную C#.

* Неоднозначность
* Регистронезависимость
* Островные языки и конструкции
* Парсинг фрагментов кода
* Скрытые токены
* Препроцессорные директивы
* Обработка и восстановление от ошибок
* Использование парсеров в IDE
* Производительность

# Неоднозначность

Неоднозначностью называется ситуация, в которой какие-то структуры языка могут
быть распознаны несколькими способами. Например, существует неоднозначность на
уровне токенов, при которой токен может являться как идентификатором, так и
ключевым словом.

На этом слайде показан пример такой неоднозначности.
`var var = 0;`

В ANTLR и других генераторах парсеров такая проблема решается добавлением
токена var в список возможных идентификаторов примерно следующим образом:

```ANTLR
// Lexer
VAR: 'var';
ID:  [0-9a-zA-Z];

// Parser
varDeclaration
    : VAR identifier ('=' expression)? ';'
    ;

identifier
    : ID
    // Other conflicted tokens
    | VAR;
```

# Объектный конструктор в C#

Roslyn способен разруливать и более сложные вещи, например, инициализацию
свойств в объектом конструкторе, при котором он понимает, что слева находится
свойство создаваемого объекта, а справа - сам объект. На этом слайде показан
пример.

# Какой результат возвращает `nameof`?

Также хочу упомянуть об еще одной особенности парсинга языковой конструкции `nameof`
в C# 6 и выше. Вернемся к нашему классу `Foo` и его свойству `Bar`. Как думаете,
что вернет `nameof` в этом коде?

<!-- Ожидание реакции и ответов из зала -->

# 42

Ну на самом деле он может возвращать здесь все что угодно, например, 42.
Дело в том, что одним из требований новых версий C# является обратная совместимость.
А такой метод в теории где-то может быть определен. Поэтому он и используется.
А иначе все методы `nameof` в старом коде перестали бы работать. То же касается
слов `async`, `await` и некоторых других.

<!-- TODO: Возможно добавить еще интересных примеров -->

# Неоднозначность: решение с использованием вставок кода

Возвращаясь к генератору парсеров ANTLR, проблему с отделением ключевых
слов от идентификаторов можно решить и с помощью вставок кода на целевом
языке, которые называются **действиями** или **семантических предиктов**. Они
используются в случае, если мощности контекстно-свободной грамматики не достаточно
для парсинга каких-то конструкций или требуется выполнять какие-то действия во
время парсинга. Обособляются они фигурными скобками. Разница между действиями и
предикатами заключается в том, что последние должны возвращать какой-то результат
и для них используется вопросительный знак в конце. Вот так выглядит простейший
пример использования таких предикатов:

```ANTLR
// Lexer
ID:  [0-9a-zA-Z];

// Parser
varDeclaration
    : id {_input.Lt(-1).Text == VAR}? id ('=' expression)? ';'
    ;

id
    : ID;
```

Как видим, в загадочной конструкции `_input.Lt(-1)` проверяется текст последнего
токена и если он равен "VAR", то для парсинга декларации используется
соответствущая альтернатива. Таким образом можно считать некоторые слова
ключевыми но только во время парсинга, а не на этапе токенизации.

# Контекстно-зависимые конструкции (PT.PM)

Все действия, которые не касаются парсинга, рекомендуется прописывать в
специальных классах, которые реализуют шаблоны проектирования **Visitor** или
**Walker**, по-другому **Listener**. Однако бывают ситуации, когда без вставок
кода просто никак не обойтись. Например, при парсинге конструкций Heredoc в PHP.

Данная конструкция определяет многострочную строку. Она интересна тем, что
начинается со специального маркера и должна заканчиваться точно таким же маркером.
Формально говоря, парсинг зависит не только от типа токена, но и от его значения.
В примере на данном слайде таким маркером является `HeredocIdentifier`.

```PHP
<?php
    echo <<< HeredocIdentifier
Line 1.
Line 2.
HeredocIdentifier
;
```

В данном случае невозможно переделать грамматику так, чтобы она без вставок
кода парсила эту строку. Решение можно посмотреть на официальном репозитории
грамматик [PHP](https://github.com/antlr/grammars-v4/blob/master/php/PHPLexer.g4).

Можно задаться вопросом: а зачем вообще разработчики PHP все так усложнили?
Удобно, когда строки нормально выглядят чисто и не содержат захламляющие символы
экранирования. Однако это получается не всегда и по крайней мере надо как-то
обозначать маркеры начала и окончания строк. Крайне маловероятно, что в Heredoc
попадется строка с `HeredocIdentifier`, однако если попадется, то можно просто
использовать другой идентификатор.

# Интерполируемые строки в C# 6

Но PHP является не единственным примером. Начиная с 6 версии в C# появилась
**интерполяция строк**. Как мы уже знаем в строке может находиться не только
само выражение, но и описание формата вывода (выравнивание, количество нулей
после запятой и т.д.). И парсить такие строки довольно сложно, потому что, например,
закрывающаяся фигурная скобка может означать как часть интерполируемого выражения
(interpolation-expression), так и выход из самого режима выражения. А двоеточие
может также быть частью выражения, а может означать конец выражения и описание
формата вывода (например, #0.##). Кроме того, такие строки могут быть как
обычными (regular), так и без экранирующих символов (verbatium), а также могут
быть вложенными друг в друга. Подробнее синтаксис описан в
[MSDN](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/interpolated-strings).

```
s = $"{p.Name} is \"{p.Age} year{(p.Age == 1 ? "" : "s")} old";
s = $"{(p.Age == 2 ? $"{new Person { } }" : "")}";
s = $@"\{p.Name}
                       ""\";
s = $"Color [ R={func(b: 3):#0.##}, G={G:#0.##}, B={B:#0.##}, A={A:#0.##} ]";
```

В C# Antlr парсере интерполяция строк была реализована с использованием стека для
подсчета уровня текущей интерполируемой строки, а также фигурных скобок. Все это
реализовано в
[CSharpLexer.g4](https://github.com/antlr/grammars-v4/blob/master/csharp/CSharpLexer.g4).

# Лексерный хак: плохое решение

В теории парсинга есть интересная [задачка](https://en.wikipedia.org/wiki/The_lexer_hack).

<!-- TODO: доделать -->

`specifier` описывает тип и может являться как ключевым словом, например `static`,
`int`, так и произвольным типом, например `MyType` как на этом слайде. Правило
`initDeclaratorList` также может являться идентификатором. Однако как минимум
одна инициализация должна присутствовать, но парсер работает жадно и распознает
при такой грамматике все токены как специфиры, поэтому в визиторе приходится
переделывать последний тип, а именно `staticGlobalVar` в переменную. Это
выглядит громоздко и некрасиво. Ситуацию еще осложняет то, что в Objective-C
поддерживается парсинг объявлений переменных без сами идентификтаоров переменных,
например [`unsinged int;`](https://goo.gl/kKhdCh). Т.е. просто так знак вопроса
из грамматики не убрать. Однако данное правило все же можно переписать в
одну строчку таким образом, чтобы парсер справлялся со всеми этими проблемами.
Кто-нибудь выскажет предположение как?

<!-- Ожидание ответов из зала -->

# Лексерный хак: изящное решение (Swiftify)

Вот это решение.

Благодаря тому, что ANTLR обладает мощным алгоритмом парсинга LL(*), который
способен анализировать произвольное количество токенов для корректного
распознавания, правило выше для парсинга деклараций можно переписать следующим образом:

```
varDeclaration
    : (specifiers initDeclaratorList | specifiers) ';'
    ;
```

Интересно, что запись из прошлого слайда и из этого не эквивалетны, несмотря
на то, что оператор ? раскрывается по сути в такую же конструкцию.
Первое правило всегда будет в приоритете, а второе будет использоваться в
случае, если распарсить первой альтернативой никак не удалось.

# Регистронезависимость

Как известно, некоторые языки программирования являются нечувствительными к
регистру. Кстати, я заметил, что это в основном характерно для языков, изначально
предназначенных для простых пользователей и учебных. Например, диалектов SQL
(это T-SQL, PL/SQL, MySQL) и Delphi.

В ANTLR нечувствительность к регистру входного потока можно достигнуть двумя
способами:

* Использовать **фрагментные токены**. Фрагментные токены отличаются от обычных
  тем, что они служат кирпичиками, из которых составляются реальные токены. А при
  токенизации они никак не учитываются. Т.е. они используются для упрощения записи.
* Использовать средства рантайма.

```ANTLR
Abstract:           A B S T R A C T;
BoolType:           B O O L E A N | B O O L;
BooleanConstant:    T R U E | F A L S E;

fragment A: [aA];
fragment B: [bB];
```

Без *фрагментных токенов* грамматика бы выглядела более громоздко:

```ANTLR
Abstract:           [Aa][Bb][Ss][Tt][Rr][Aa][Cc][Tt];
```

# Регистронезависимость: решение на уровне рантайма (PT.PM)

Также можно нормализовать регистр входного потока символов и далее уже использовать
стандартные токены. Недостатком является то, что информация о регистре полностью
удаляется, а она может быть иногда важна. Например, в том же PHP ключевые слова
являются нечувствительными к регистру, а названия переменных - чувствительными.
Например, `$id` и `$ID` - разные переменные. К счастью, ANTLR поддерживает
специальный [входной поток](https://gist.github.com/sharwell/9424666)
сохраняющий регистр у значений токенов, но при этом не учитывающий его при
лексическом анализе. Код лексера становится еще чище, а скорость парсера еще быстрее.
С помощью такого подхода сейчас и написаны грамматики SQL в официальном репозитории.

# Островные языки и конструкции (PT.PM)

Существуют языки, фрагменты которых вложены в другие языки. Такие языки называются
**островными**, а типичными примерами являются JavaScript внутри PHP или,
например, вставки C# внутри Aspx.

Может возникнуть вопрос: Зачем эти языки парсить вместе? Ведь сначала на сервере
отрабатывает парсер PHP, а затем на клиенте в браузере парсится JavaScript без
единого php блока. Однако иногда удобно парсить все в одном месте, например, для
статических анализаторов кода.

```PHP
<?php
<head>
  <script type="text/javascript">
    document.body.innerHTML="<svg/onload=alert(1)>"
  </script>
</head>
```

При парсинге таких языков требуется использовать как разные типы токенов,
так и разные типы синтаксических структур.

К счастью, в ANTLR и других генераторах парсеров существует возможность
переключения режимов распознавания лексем по определенной последовательности
символов или условию. В примере выше такими маркерами являются открывающий и
закрывающий теги `<script>` с определенными аттрибутами.

После этого для полученного потока токенов последовательно используется два
парсера. Сначала парсится код на PHP, при этом вставки JS воспринимаются как строка,
а затем, на этапе обхода дерева разбора PHP, парсятся вставки JS кода, причем
к каждому обработанному узлу добавляется смещение относительно текущего
фрагмента кода.

Кстати, усилиями Positive Technologies была взята из Mono и
доработана библиотека [AspxParser](https://github.com/PositiveTechnologies/aspxparser)
для парсинга Aspx файлов. Она распространяемая по лицензии MIT.

# PHP: альтернативный синтаксис (PT.PM)

Вот еще пример кода на PHP, в котором используется так называемый
*альтернативный синтаксис* и блоки HTML перемешиваются с блоками PHP кода.
Если кто не понял, то здесь "запрятана" конструкция `switch case`.

```PHP
<?php switch($a): case 1: // without semicolon?>
        <br>
    <?php break ?>
    <?php case 2: ?>
        <br>
    <?php break;?>
    <?php case 3: ?>
        <br>
    <?php break;?>
<?php endswitch; ?>
```

# Использование отдельного режима лексем для JavaScript (PT.PM)

В ANTLR переключение режима лексем осуществляется с помощью команд
`pushMode`, `mode`, `popMode` и выглядит это примерно так:

```
ScriptText:   ~[<]+;
ScriptClose:  '</script'? '>' -> popMode;
PHPStart:     '<?=' -> type(Echo), pushMode(PHP);
PHPStart:     '<?' 'php'? -> channel(SkipChannel), pushMode(PHP);
ScriptText2:  '<' ~[<?/]* -> type(ScriptText);
ScriptText3:  '?' ~[<]* -> type(ScriptText);
ScriptText4:  '/' ~[<]* -> type(ScriptText);
```

В этом фрагменте лексера также используются команды `type`, которая изменяет
тип токена и `channel`, которая помещает токен в отдельный изолированный канал.

# Обработка комментариев и пробелов

Важным критерием при выборе парсера является обработка скрытых токенов, т.е.
токенов, которые не играют никакой значимой роли для компилятора, однако
помогают разделять другие токены или служат полезными для программистов.
Такими токенами обычно являются пробелы и комментарии. Такие токены можно
помещать в поток скрытых токенов, которые не будут мешаться при парсинге.
Однако в этом случае теряется связанность токенов с деревом разбора, что может
быть критично при трансформациях дерева, например, при рефакторинге. Если
говорить про грамматики, то можно попытаться связать такие токены с основными
включив их токены непосредственно во все правила примерно следующим образом:

```ANTLR
declaration:
    property COMMENT* COLON COMMENT* expr COMMENT* prio?;
```

Понятное дело, что такое решение никуда не годится. Грамматика станет
громоздкой и избыточной, т.к. повсюду будут мельтешить комментарии и пробелы.

В связи с этим было проанализировано два подхода по связыванию скрытых токенов
с узлами дерева разбора:

* Первый заключается в связывании скрытых токенов с нетерминальными узлами
  дерева разбора.
* А второй - в связывании с терминальными узлами или токенами.

# Связывание скрытых токенов с узлами дерева (Swiftify)

В нашем сервисе Swiftify используется пока что первый способ. Была найдена
реализация для ANTLR 3 и доработана. В этом случае множество токенов разбивается
на предшествующие, последующие и сироты.

Рассмотрим следующий пример с двумя выражениями присваивания и фигурными скобками.
**Предшествующие** (Precending) токенами располагаются перед узлами присваивания.
В этом множество также попадает и начальный комментарий в документе (`//First Comment`).

```
//First comment
'{' /*Precending1*/ a = b; /*Precending2*/ b = c; '}'
```

В случае, если после токена существует только терминальный символ, он заносится
во множество **последующих** токенов (Following). Последний комментарий также
попадает в это множество.

```
'{' a = b; b = c; /*Following*/ '}' /*Last comment*/
```

Все остальные скрытые токены, которые располагаются между двумя значимыми токенами,
попадают в список **сирот** (Orphans).

```
'{' /*Orphan*/ '}'
```

Более подробное о таком связывании написано в статье
[Tackling Comments in ANTLR Compiler](http://meri-stuff.blogspot.ru/2012/09/tackling-comments-in-antlr-compiler.html).
В данном способе есть недостаток: скрытые могут быть связаны со всеми типами
нетерминальных узлов, и обрабатывать их не очень удобно.

# Связывание скрытых токенов со значимыми (Roslyn)

Другой подход заключается в связывании скрытых токенов с терминальными узлами
дерева разбора или значимыми токенами. Такой подход реализован в Roslyn.
Синтаксическое дерево Roslyn имеет три типа узлов:

* **Node** - по сути нетерминальный узел дерева разбора, который хранит в себе
  несколько других узлов и отображающий определенную конструкцию.
* **Token** - терминальный узел дерева, значимый токен. Представляет ключевое
  слово, идентификатор, литерал или пунктуацию. Например, число `42` или `;`.
* **Trivia** - токен, представляющий комментарии, пробелы и другие скрытые токены.
  Он не является узлом дерева, однако связывается с токенами. Такие узлы
  оказываются незаменимыми при трансформации дерева обратно в код
  (например, для рефакторинга).Сами тривии могут быть:
  * Лидирующие (**Leading**)
  * Замыкающие (**Trailing**)

Во множество замыкающих токенов попадают все тривии на той же самой строчке от
значимого токена до следующего значимого токена. Все остальные скрытые токены
попадают в множество лидирующих и связываются со следующим значимым токеном.
Первый значимый токен содержит в себе начальные тривии файла. Скрытые токены,
замыкающие файл, связываются с последним специальным end-of-file токеном нулевой длины.

Для лучшего понимания я подготовил пример кода на C#, в котором наглядно
представлены лидирующие и замыкающие тривии.

```CSharp
// leading 1 (var)
// leading 2 (var)
var foo = 42; // trailing (;)

// leading (int)
int bar = 100500; // trailing (;)

// leading (EOF)
EOF
```

Такое связывание позволяет лучше формализовать положение скрытых токенов и даже
отключенного кода из препроцессорных директив!

# Препроцессорные директивы (Roslyn)

Напомнию, что *препроцессорными директивами* назваются вставки на специальном
языке, с помощью которого можно отключать куски кода при определенных условиях.
Например, их часто используют для использования разных методов API в разных
версиях .NET фреймворков.

Рассмотрим такой пример:

```CSharp
bool trueFlag =
#if NETCORE
    true
#else
    new Random().Next(100) > 95 ? true : false
#endif
;
```

В этом случае при заданном символе `DEBUG` отключенный (задизабленный) код
попадет в список *Leading* для точки с запятой `;`, а текст `#if NETCORE` - в
список *Leading* для токена `true`.

<!-- TODO: проверить на всякий случай -->

# Препроцессорные директивы: одноэтапная обработка (Swiftify)

Если проводить аналогию с ANTLR, то токены директив заносятся в специальный
канал. При обходе дерева разбора директивные токены учитываются и парсятся.
Такой подход позволяет интерпретировать и обрабатывать макросы как обыкновенные
функции (при удалении директив макрос бы просто инлайнились):

```Objective-C
#define DEGREES_TO_RADIANS(degrees) (M_PI * (degrees) / 180)
```

```Swift
func DEGREES_TO_RADIANS(degrees: Double)
    -> Double { return (.pi * degrees)/180; }
```

Попробовать в реальном времени это уже можно в нашем веб-сервисе по конвертингу
Objective-C кода ([Swiftify](http://objc.to/l4bamg)). Однако в этом случае
возможны потенциальные ошибки парсинга для незаконченных фрагментов кода.

# Препроцессорные директивы: двухэтапная обработка (Codebeat)

Если говорить о классическом способе обработки препроцессорных директив, то
сначала обрабатывается код директив, а затем уже используется обычный парсер.
Например, для кода выше входной код для парсера будет выглядеть следующим образом.

```CSharp
bool·trueFlag·=
·········
····true
·····
··············································
······
;
```

Обратите внимание, что пробелы все сохраняются, т.к. они нужны для корректного
отображения ошибок и локации (TextSpan) у всех реальных узлов.

Последовательность действий при этом способе такая:

1. Токенизация и разбор кода препроцессорных диреткив.
2. Вычисление условных директив `#if` и определение компилируемых блоков кода.
3. Замена директив из исходника на пробелы.
4. Токенизация и парсинг результирующего текста с удаленными директивами.

Такой способ двухэтапной обработки также реализован для грамматики Objective-C
и используется в веб-сервисе для подсчета метрик кода и проведения ревью
[Codebeat](https://codebeat.co/).

# Парсинг фрагментов кода (Swiftify, PT.PM)

Входной единицей компилятора обычно является целый файл, т.е. если говорить
про C#, то такой файл в начале содержит список `using`, затем идет объявление
`namespace`, декларация класса, ну и т.д. Т.е. известно с чего начинается парсинг.
Если говорить в терминах ANTLR и грамматик, то известно правило парсинга, обычно
это что-то вроде `topDeclaration` или `compilationUnit`.

Однако что если нужно правильно распарсить фрагмент кода? Например, такое
может использоваться в IDE для рефакторинга или, например, для конвертинга
кусочка кода. Задача усложняется если в этом фрагменте еще есть и ошибки.

Эту задачу можно решить разными способами: с использованием операций на строках и
регулярных выражений, с помощью токенов и полноценного парсинга. Можно пытаться
определить правило парсинга по ключевым словам, однако они могут встретиться и
в комментариях. С токенами тоже не все так гладко. В итоге было разработано
решение, которое заключается в парсинге фрагмента разными правилами и выборе правила
без ошибок или максимально близкой к концу позицией ошибки. Пробовали выбирать
правила с минимальным количеством ошибок, но это показало худшие результаты.

По ссылкам доступны примеры таких фрагментов, ну вы и сами можете попробовать
ввести свои и потестить на сервисе.

* [Утверждения](http://objc.to/6mpmhq)
* [Декларации методов](http://objc.to/nt25a1)
* [Свойства](http://objc.to/vnpasw)

# Ошибки парсинга

Кстати, об ошибках. Важной способностью каждого парсера является обработка ошибок,
т.к. парсер не должен падать только из-за одной ошибки, а должен "понимать",
что это за ошибка и парсить код дальше. Он должен возвращать минимальное количество
ошибок отображать наиболее понятное описание. Тут можно вспомнить про C++ с кучей
ошибок в случае неправильно написанного шаблона.

У Roslyn и ANTLR примерно одинаковые коды ошибок, но последний конечно хуже
с ними справляется из-за своей универсальности.

В случае, если лексер ANTLR не может подобрать лексическое правило для входного
символа, то генерируется **ошибка распознавания токена**. Плохо то, что
в случае кракозябр будет тьма ошибок, равная количеству этих кракозябр. В
Roslyn такого не будет. Однако с помощью специального правила
`ERROR: . -> channel(ErrorChannel)` можно помещать все такие ошибки в отдельный канал.

Вот пример кода с лексической ошибкой:

```CSharp
class # { int i; }
```

Все остальные ошибки касаются парсера. Часто случаются ситуации, в которых
разработчик не дописал нужный символ или наоборот, написал лишний символ.
В этом примере вверху пропущена скобка, а внизу добавлена лишняя точка с запятой
после идентификатора класса. Такие ошибки назвают ошибками с единичным
отсутствующим и лишним токеном.

```CSharp
class T { int f(x) { a = 3; } // Отсутствующий токен
class T ; { int i; } // Лишний токен
```

В случае если лишних токенов несколько, то в парсере активируется режим
"паники" и он пытается пропустить лишние токены, пока не встретит конечный
синхронизирующий. Таким может являться, например, `;` или `}`. Вот простеший
пример:

```CSharp
class T { int f(x) { a = 3 4 5; } }
```

Все остальные ошибки классифицируются как отсутствующее альтернативное
подправило:

```CSharp
class T { int ; }
```

# Ошибки парсинга в Roslyn

На этом слайде показан пример C# кода, в котором удалось всевозможные синтаксические
ошибки Roslyn:

* Ошибка **недостающий синтаксис**; В этом случае Roslyn достраивает
  соответствующий узел со значением свойства `IsMissing = true` (типичным пример
   — Statement без точки с запятой);
* **незавершенный член**; создается отдельный узел `IncompleteMember`;
* **некорректное значение численного, строкового или символьного литерала**
  (например, слишком большое значение, пустой char): отдельный узел с Kind, равным
  `NumericLiteralToken`, `StringLiteralToken` или `CharacterLiteralToken`;
* **лишний синтаксис** (например, случайно напечатанный символ): создается
  отдельный узел с `Kind = SkippedTokensTrivia`.

Как видим, ошибки *недостающий синтаксис* (Missing) и *лишний синтаксис*
(Skipped Trivia) похожи на аналогичные ошибки в ANTLR.

```CSharp
namespace App
{
    class Program
    {
        ;                    // Skipped Trivia
        static void Main(string[] args)
        {
            a                // Missing ';'
            ulong ul = 1lu;  // Incorrect Numeric
            string s = """;  // Incorrect String
            char c = '';     // Incorrect Char
        }
    }

    class bControl flow
    {
        c                    // Incomplete Member
    }
}
```

Как уже говорилось в начале, парсеры, написанные вручную, более гибкие. Это
касается в том числе и восстановления от ошибок. Видите, здесь даже подсветка
поплыла, а Roslyn выдал всего 6 ошибок!

Благодаря таким продуманным типам синтаксических ошибок и связыванию скрытых
токенов, Roslyn может преобразовать дерево с любым количеством ошибок обратно в
код символ в символ. Такое дерево называют **достоверным** (**full fidelity**).

# Заключение о парсинге

Я затронул не все темы парсинга, освятить всплывающие подсказки, форматирование
кода, оптимизацию. Но так как у нас ограниченное время, то хочу еще немного
освятить тему обработки древовидных структур в .NET.

# Обработка древовидных структур

Как я уже говорил, парсер возвращает древовидную структуру, которая называется
*деревом разбора*. Абстрактное синтаксическое дерево или AST называют дерево
разбора на более высоком уровне, из которого удалены не значимые токены,
такие как скобки, запятые.

После того, как получено любое из деревьев его нужно как-то обрабатывать.
Сложность представляет сообой то, что эта структура иерархическая и для ее обхода
используются специальные шаблоны проектирования, которые называются *Visitor* и
*Listener*. Эти шаблоны можно проектировать и реализовывать по-разному.
Помимо обхода, деревья еще можно сериализовывать, т.е. сохранять их состояние для
для последующего использования или передачи. Также при различных операциях с
деревом нередко всплывают вопросы производительности.

# Методы обхода деревьев

В Visitor для обработки потомков какого-либо узла необходимо вручную вызывать
методы их обхода. При этом если родитель имеет три потомка, и вызвать методы
только для двух узлов, то часть поддерева вообще не будет обработана.
В Listener (Walker) же методы посещения всех потомков вызываются автоматически.
В Listener существует метод, вызываемый в начале посещения узла (`EnterNode`) и
метод, вызываемый после посещения узла (`ExitNode`). Эти методы также можно
реализовать с помощью механизма событий. Методы Visitor, в отличие от Listener,
могут возвращать объекты и даже могут быть типизированными, т.е. при объявлении
`CSharpSyntaxVisitor` каждый метод Visit, будет возвращать объект AstNode, который
в нашем случае является общим предком для всех остальных узлов AST.

Таким образом, при использовании паттерна проектирования Visitor, код
преобразования дерева получается более функциональным и лаконичным за счет того,
что в нем нет необходимости хранить информацию о посещенных узлах. На рисунке
ниже можно увидеть, что, например, при преобразовании языка PHP, ненужные узлы
HTML и CSS отсекаются. Порядок обхода обозначен числами. Listener обычно
используется при агрегации данных (например из файлов типа CSV). Ну и в принципе
любой Listener является частным случаем визитора, поэтому на нем я далее
останавливаться не буду.

![Visitor & Listener](https://habrastorage.org/files/bd1/69c/535/bd169c535e854f9681520f520d0db9c3.png)

# Реализации Visitor

Посетители и слушатели могут быть написаны вручную, сгенерированы, а также
разработаны с использованием рефлексии и DLR, т.е. ключевого слова `dynamic`.

Код, написанный вручную, опять-таки предоставляет максимальную гибкость и
производительность. Однако размер исходника напрямую зависит от количества
узлов в обрабатываемом дереве, ну и такие визиторы долго и утомительно разрабатывать.
А чем больше код, тем легче в нем ошибиться. Пример визитора с кодом, написанным
вручную я показывать не буду, потому что он достаточно тривиальный.

Также визиторы могут быть очень быстро сгенерированы. В ANTLR, например, для каждого
правила грамматики генерирется отдельный метод Visit. Однако не всегда есть
возможность генерации, также недостатком является то, что код внутренностей
визиторов хуже воспринимается, избыточный, и он может нарушать какой-то
проектный Code Style. Например, если слова в правилах разделяются символами
подчеркивания, то в названиях методов Visit также будут некрасивые подчеркивания.

# Диспетчеризация с использованием рефлексии

Разумным компромиссом является третий способ, а именно, визитор с использованием
рефлексии и ключевого слова `dynamic`. Обход деревьев и других графовых структур,
кстати, является наглядной демонстрацией того, где можно применять это слово, если до
этого вы его нигде не исползовали.

Суть данного способа заключается в том, что логика обхода узлов по-умолчанию
прописывается не в каждом методе `Visit`, а в одном методе `VisitChildren`,
который извлекает свойства для каждой ноды и производит их обход. Интересным
моментом является и динамическая диспетчеризация с использованием
`dynamic`. Например, в этом коде DLR определит метод `Visit`, который нужно
посетить для объекта `customNode`: `ExpressionStatement`, `StringLiteral` или
каким-то другим аргументом.

```CSharp
// invocation in VisitChildren
Visit((dynamic)customNode);

// visit methods

public virtual T Visit(ExpressionStatement expressionStatement)
{
    return VisitChildren(expressionStatement);
}

public virtual T Visit(StringLiteral stringLiteral)
{
    return VisitChildren(stringLiteral);
}
```

Такая реализация может иметь какие-то проблемы с производительностью, однако
хорошо подходит для прототипов. Мы в Positive Technologies, например, сейчас
используем такие визиторы при разработке универсального анализатора кода на
основе потока данных или Taint движке. Простой пример реализации можно найти
в проекте [TreeProcessing.NET](https://github.com/KvanTTT/TreeProcessing.NET).

# Архитектура Visitor

Последняя вещь, которую я хотел бы упомянуть о визиторе - это их реализация.
Можно использовать несколько классов-визиторов, которые будут отвечать за обработку
своих узлов, например, `Declaration`, `Expression`, `Statmement` и другие.
А можно использовать один большой визитор, в котором будут прописаны визиторы
для всех узлов.

В Swiftify мы сначала использовали первый подход, но потом перешли на второй, т.к.
это позволило упростить код и избавиться от лишних аллокаций ненужных визиторов.
К счастью, C# поддерживает частичные классы, что позволило разбить реализацию
такого гиганского визитора на несколько файлов. Однако и первый подход имеет
свои преимущества по сравнению с единым: для каждого визитора можно определить
свой возвращаемый тип. Например, визиторы для выражений могут возвращать какие-то
выражения, визитор для методов и классов может возвращать их декларации.
Однако для нас это оказалось не принципиальным, поскольку при конвертинге при
кода на выходе всегда получается строка.

Более того такой визитор позволил лучше формализовать процесс обхода дерева,
т.к. в нем переопределяются все методы и для узлов, которые точно не должны
посещаться, используется заглушка `throw new ShouldNotBeVisitedException(context);`.

# C\# 7: локальные функции

Как известно, последней версией C# является 7. В нем появилось достаточно
много нововведений, таких как out переменные, Pattern Matching в том числе включая is,
кортежи, локальные функции, улучшения записи литералов и другие фичи.

Некоторые фичи прямо все давно ждали и они облегчают запись кода, а применение
других не совсем очевидно, например, локальные функции. На мой взгляд, такие
функции может и не так сильно сокращат запись, зато хорошо организуют рекурсию.
У рекурсии есть инициализация и основное действие. На этом слайде показан
код метода, возвращающий список терминальных узлов (листьев) для переданного узла.
При инициализации создается список, а при обходе он заполняется. Без локальных
функций пришлось бы писать вторую функцию, которая использовась бы только в этом
месте.

```CSharp
public static List<Terminal> GetLeafs(this Rule node)
{
    var result = new List<TerminalNode>();
    GetLeafs(node, result);

    // Local function
    void GetLeafs(Rule localNode, List<Terminal> localResult)
    {
        for (int i = 0; i < localNode.ChildCount; ++i)
        {
            IParseTree child = localNode.GetChild(i);
            // Is expression
            if (child is TerminalNode typedChild)
                localResult.Add(typedChild);
            GetLeafs(child, localResult);
        }
    }
    return result;
}
```

Конечно в C# есть и лямбда функции, однако они вызываются через делегат, а значит
менее быстрые, они не могут быть рекурсивными, generic и в конце концов хуже
выглядят.

<!-- TODO: Пример с двумя функциями, локальной и делегатами -->

# Сериализация

Форматы сериализации бывают следующими:

* Бинарная
* XML, JSON, YAML и другие языки разметки
* Собственный формат

Как известно, бинарная сериализация очень быстрая, но формат нечитаем для человека.
XML и Json форматы наоборот, удобны для чтения и редактирования, однако отстают
по скорости от бинарной. Под .NET существуте большое количество библиотек
сериализации, такие как Protobuf, MessagePack, Json.NET, а также встроенные
во фреймворк. Подробно я на них останалвиваться не буду, однако поделюсь ссылкой
на недописанную с 2015 статью, в которой подробно рассматривается сериализация
древовидных структур и другие вещи: [Сериализация, отображение, сравнение и обход древовидных структур в .NET](https://github.com/KvanTTT/Articles/blob/tree-structures-serialization-comparison-and-mapping-on-net/Tree-Structures-Serialization-Comparison-and-Mapping-on-NET/Russian.md).
А реализация находится в вышеупомянутом проекте [TreeProcessing.NET](https://github.com/KvanTTT/TreeProcessing.NET).

К сожалению, не все библиотеки умеют работать с деревьями из коробки и иногда
требуется доработка напильником, чтобы сериализация вообще работала, результат
был независим от рантайма и был максимально приятен для человека.

# JSON сериалзиация деревьев

Я расскажу о том, как мы приручили популярную библиотеку Json.NET для такого
вывода. Дело в том, что если свойство имеет абстратный класс, который
конкретизируется во время исполнения, но, понятное дело, необходимо записывать
информацию об этом конкретном типе для последующей корректной десериализации.
Например, свойство `Expression` может конкретизироваться `BinaryOperatorExpression`
или `StringLiteral`. Json.NET поддерживает из коробки параметр `TypeNameHandling.All`
для обработки типов, однако результирующий JSON выглядит многословным и зависимым
от рантайма (т.е. `System.Collections.Generic.List` - дотнетовская сущность).

```JSON
"Statements": {
    "$type": "System.Collections.Generic.List`1[[TreesProcessing.NET.Statement, TreesProcessing.NET]], mscorlib",
    "$values": [
        ...
    ]
```

# Своя логика сериализации с JsonConverter

К счастью, Json.NET поддериживает класс, позволяющий переопределить
логику сериализации и десериализации определенных узлов, `JsonConverter`.

В качестве идентификатора типа конкретного класса можно использовать как само
имя класса, так и самостоятельно определенное свойство или аттрибут.
Дополнительным плюсом Enum свойства является то, что его можно использовать не
только при сериализации, но и для более быстрого сравнения двух узлов без
использования операторов приведения, что также можно использовать.

* **Имя класса** в качестве идентификации типа
  ```CSharp
  BinaryOperatorExpression
  ```
* **Свойство** в качестве идентификации типа
  ```CSharp
  override NodeType NodeType => NodeType.InvocationExpression;
  ```
* **Аттрибут** в качестве идентификации типа
  ```CSharp
  [NodeAttr(NodeType.BinaryOperatorExpression)]
  public class BinaryOperatorExpression : Expression
  ```

При десериализации используется `Activator`, который создает существующий
в сборке класс по переданной строке.

Результирующий JSON при таком подходе выглядит покороче:

```JSON
"Statements": {
    "NodeType": "BlockStatement",
    "Statements": [
        ...
    ]
```

# Оптимизация (Swiftify)

При обходе деревьев часто приходится обращаться к одним узлам по несколько раз.
Хорошо, если эти узлы - ближайшие, т.е. дети или родители. Однако если узел
расположен через несколько поколений, да еще и ищется по определенному условию,
то большое количество обращений к ниму может повлечь проблемы с производительностью,
особенно если файл большой. Для решения этих проблем можно кэшировать
информацию о посещенных ранее узлах для быстрого извлечения в дальнейшим. Такой
примем называется **мемоизацией**.

В Swiftify у нас есть метод, который возвращает первого потомка определенного
типа `FirstDescendantOfType`. После записи списка наследников для каждого
узла удалось добиться увеличения производительности более чем в два раза для
большого файла. Однако ничего не бывает бесплатным: потребление памяти
возросло примерно в 3 раза. Однако в нашем случае потребление памяти все равно
было небольшим, так что это оказалось предпочтительней.

Также целесообразно избавляться от аллокаций объектов в методах, которые
часто вызываются. У нас таких методом является `Visit`: он вызывается
практически для каждого узла дерева разбора, т.к. содержит в себе логику
обработки скрытых токенов, а также обработчик исключений. Раньше там всегда
создавался `StringBuilder`, даже если child был один. Ну т.е. понятно, что можно
сразу его результат и возвращать. Данная оптимизация тоже немного уменьшила
потрбление памяти и ускорила скорость.

# Ресурсы

Я заканчиваю доклад. На этом слайде предоставлен список ресурсов, на основе
которых готовился материал. Как я уже говорил, практически все можно найти
на гитхабе под пермиссивными лиценизями, а сама презентация также доступна там.

# Вопросы?

Теперь я готов ответить на ваши вопросы.
<!-- Ответы на вопросы-->