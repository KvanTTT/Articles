# Теория и практика формальных языков

Привет всем! Меня зовут Иван Кочуркин. Я работаю в Positive Technologies
над универсальными анализаторами исходных кодов и подрабатываю в Swiftify,
веб-сервисе для преобразования кода Objective-C в Swift. Также я веду
активную деятельность на GitHub под ником KvanTTT и пишу статьи на хабре на
техническую тематику.

Вся моя работа и хобби так или иначе связаны с парсингом как небольших предметно-
ориентированных языков (т.е. DSL), так и полноценных языков программирования.

Например, в Positive Technologies я занимался и занимаюсь разработкой языка описания шаблонов,
предназначеных для описания различных недостатков и уязвимостей в исходном коде.
Также я занимался разработкой парсеров для языков Java, PHP, SQL языков и Objective-C.

В этом докладе я хотел бы рассказать о базовых понятиях парсинга, о том, какие
библиотеки парсинга существуют под платформу .NET и язык C#. А потом я хотел бы
углубиться в проблемы парсинга на примере и сравнении таких инструментов как
ANTLR и Roslyn. В завершающей части доклада я коснусь темы обработки результатов
парсинга, т.е. деревьев разбора.

Сразу предупрежу, что доклад не совсем связан с .NET, однако практически
весь материал носит прикладное значение и используется как в открытых, так и в
коммерческих проектах. Более того, большую часть материала можно найти на гитхабе!

# Почему не Regex?

Итак, начнем со всеми известными и любимыми регулярными выражениями. Почему их
не достаточно для парсинга всего? Я думаю большая часть аудитории так или иначе
догадываются. И, несмотря на то, что современные регулярки, например в C#, обладают
мощностью полноценных языков, т.е. контекстно-свободных, описывать ими
конструкции с определенного этапа становится очень сложно.

Рассмотрим такую регулярку с открывающим и закрывающим тегом table: `<table>(.*?)</table>`.
Данное выражение парсит не жадно текст до закрывающегося тега `</table>`.
Ок, все хорошо, все довольны. А что, если нужно добавить еще и обработку аттрибутов?
Ок, нет проблем используем из пункта 2: `<table.*?>(.*?)</table>`. А что если дальше нужно добавить
поддержку элементов `<tr>` и `<td>`, которые внутри себя также могут содержать
кучу тегов? Или, допустим, добавить поддержку комментариев `<!-- html comment -->`.
После определенного этапа вам не захочется вносить изменения в это сложное
регулярное выражение, и вы поймете, что здесь что-то не так :)

Кстати, то же самое касается и парсинга любых языков, в котором есть вложенные
элементы, например простейшие блоки из фигурных скобок в C#.

# Лексемы и токены

Ну ок, попробуем придумать способ как отфильтровать хотя бы комментарии в html.
<!-- Спросить у зала--> Как известно, комментарии начинаются и заканчиваются
определенными маркерами или последовательностью символов. Так значит
можно заранее детектировать текст заключенный в такие маркеры, удалять его или
помещать куда-то в другое место и парсить дальше как ни в чем не бывало!
Круто, но получается обработка в два прохода. А что, если сделать это в один
проход и сразу создавать структурки для таких вот последовательностей,
при этом назначая тип для каждой последовательности на этом же проходе! А помимо
комментариев так же можно определять пробелы, числа, идентификаторы, строки и другие
типы.

Тоже вполне себе подход. Более того, для таких последовательностей даже
существуют общепринятые определения. **Токеном** обозначается последовательность
определенных символов и их тип. Саму эту последовательность обычно называют
**лексемой**. <!-- Более того, сами типы токенов можно использовать в конечном автомате. -->
Процесс создания токенов из текста называется **токенизацией**.

Для описания токенизаторов существуют свои языки и инструменты. Обычно каждый
токен описывается на новой строке: слева прописан идентификатор, а справа -
его значение или лексема. На этом слайде приведен простейший пример:

```ANTLR
MyKeyword: `var`;
Id:        [a-z]+;
Digit:     [0-9]+;
Comment:   '<!--' .*? '-->';
```

# Дерево разбора и AST

Однако наша проблема с regex решена не полностью! Со скрытыми символами разобрались,
но как обрабатывать вложенные теги? Ведь токенизатор возвращает поток токенов,
а у нас рекурсивная структура!

<!-- TODO: возможно добавить фрагмент кода с рекурсией -->

Или как распарсить структуру, если ее дальнейший парсинг зависит от типа первого токена?
Так вот, для таких случаев и были разработаны разные алгоритмы и практики.
Если поток токенов представляет собой линейную последовательность, то
распознанные структуры уже более сложные: это деревья, а именно
иерархический список узлов, каждый из которых содержит в себе дочерние узлы или дети.
Такие структуры называются **деревьями разбора** или **конкретными деревьями синтаксиса** (**CST**).

Некоторые элементы в языке в принципе нужны только для корректного парсинга
и никак не влияют на код, который выдает компилятор языка, например, точка с
запятой в C#. Без комментариев и пробелов, понятное дело, тоже можно обойтись.
Так вот, *дерево разбора*, которое не содержит все такие не значимые токены,
называют великим и ужасным **абстрактным синтаксическим деревом** или **AST**.

Процесс создания деревьев разбора из потока токенов называется парсингом.
Стоит отметить, что парсеры бывают как классическими, принимающими на вход
поток токенов, так и **безлексерными**, принимающие на вход сырой исходник.
Достоинством первых является более высокая скорость работы, а вторых - гибкость.

# Типы парсеров

Парсеры бывают нескольких типов:

* Готовые библиотеки парсинга
* Парсеры, написанные вручную
* И автоматически сгенерированные парсеры из грамматик

Готовыми библиотеками парсинга являются, например, встроенный движок регулярных выражений,
парсер XML и внешние библиотеки, например хорошо известная JSON.NET, ServiceStack и другие.
Их достоинствами являются хорошее качество, производительность, т.к. они
написаны для распространенных форматов, опенсорсные и существуют продолжительное
время. Также они имеют обширный API, позволяющий, например, прописывать
произвольную логику десериализации для каких-то сложных древовидных или графовых структур.
Однако такие парсеры не получится использовать для других языков,
а тем более своих предметно-ориентированных или **DSL** (Domain Specific Language).

Вторым типом являются парсеры, написанные полностью вручную. Такие парсеры
наиболее гибкие в возможностях и имеют максимальную производительность.
Однако требуется большое погружение в предметную область.
Разработка таких парсеров очень долгая и занимает много-много человекочасов.
Примером таких парсеров под C# являются Roslyn. На самом деле он не просто парсер,
а полноценный инструмент для парсинга, анализа и компиляции C# и VB кода.
Однако даже такой мощный фреймворк заточен только на два языка, он не сможет
парсить любые нужные нам языки, да и работает он только под .NET.

Поэтому на помощь приходят такие инструменты, как **генераторы парсеров** или
**компиляторы компиляторов**. С помощью них можно генерировать парсеры сразу
под несколько языков (или рантаймов) имея только одно описание языка, которое
называет **грамматикой**. Также существуют библиотеки для создания парсеров в
рантайме, а не на этапе генерации. Они называются **комбинаторами парсеров**.

# Грамматика

В грамматике описывается набор **правил**, каждое из которых определяет
узел в дерева разбора. Правило состоит из нескольких **альтернатив**.
На этом слайде приведен простейший пример: в нем описано
правило expression для умножения, сложения, вызова функции и идентификатора.
Т.е. содержит 4 альтернативы и будет парсить текст `a + b * c` причем с
соблюдением приоритетов операций: сначала умножение, а потом - сложение.

```ANTLR
expr
    : expr '*' expr
    | expr '+' expr
    | ID '(' args ')'
    | ID
    ;

ID: [a-zA-Z]+;
```

Стоит отметить, что правило `expr` здесь ссылается само на себя в двух
альтернативах. Такие правила называются **лево-рекурсивными** и не все генераторы
парсеров способы справляться с ними, поэтому их приходится переписывать
их определенным образом. Но прежде чем я расскажу о существующих библиотеках
парсинга на C#, я опишу типы языков, которые они могут покрывать.

---

# Типы языков

Вообще говоря, существует система классификации языков, которая называется
**иерархией Хомского**. Упрощенно она выглядит так и наши любимые регулярные
выражения по ней обладают наименьшей выразительностью, а практически
весь синтаксис современных языков программирования можно описать с помощью
**контекстно-свободных** грамматик.

* Регулярные
* Контекстно-свободные
* Контекстно-зависимые
* Тьюринг-полные

Если сравнивать парсеры КС-языков и регулярных выражений «на пальцах»,
то последние не имеют памяти, точнее могут запоминать только одно состояние.
А если сравнивать парсеры КЗ- и КС-языков, то последние не запоминают значения
(не типы) посещенных ранее токенов, т.е лексем.

Кроме того, язык в одном случае может являться КС, а в другом — КЗ.
Если учитывать **семантику**, т.е. согласованность с определениями языка,
в частности, согласованность типов, то язык может рассматриваться как КЗ.
Например, `T a = new T()`. Здесь тип в конструкторе справа должен быть таким же,
какой указан слева. Но обычно целесообразно проверять семантику после этапа парсинга.

---

# Инструменты и библиотеки под C#

Под C# существует обширное количество библиотек для парсинга, охватывающие
как регулярные, так и контекстно-свободные языки, с поддержкой левой рекурсиии
и без, использующие лексер и нет, а также предоставляющие средства анализа кода.

Nitra, JetBrains MPS представляют собой языковые фреймворки, еще называют Language Workbench).
Их можно использовать для создания, расширения языков, описания семантики,
кодогенерации, интеграции языка с IDE и других сценариев. В общем эдакое решение
"все из коробки". Ну и Roslyn туда же.

Подробно я об остальных инструментах рассказывать не буду, разве что коснусь
комбинаторов. Однако все инструменты и библиотеки детально расписаны в недавней статье
[Parsing In C#: Tools And Libraries](https://tomassetti.me/parsing-in-csharp).

# Парсер-комбинаторы

Как я уже упоминал, комбинаторы парсеров встроены в язык, а значит никакая
генерация не нужна. В некоторых случаях это немного упрощает рабочий процесс, т.к.
отпадает необходимость его настраивания. Также комбинаторы могут быть
интегрированы с IDE разработчика. На практике они полезны для мелких задачек
парсинга, чуть сложнее чем регулярные выражения. В общем если вы нуждаетесь в
парсере, но не хотите сильно вникать в предметную область, то комбинаторы
парсеров - ваш выбор.

# Примеры кода парсер-комбинатора

Вот пример простейшего кода с использованием комбинатора [Sprache](https://github.com/sprache/Sprache):

```CSharp
// Parse any number of capital 'A's in a row
var parseA = Parse.Char('A').AtLeastOnce();
```

Или, например, правило для идентификатора. В нем мы видим, что лидирующие и
замыкающие пробелы игнорируются, а сам идентификатор должен начинаться с буквы.

```CSharp
Parser<string> identifier =
    from leading in Parse.WhiteSpace.Many()
    from first in Parse.Letter.Once()
    from rest in Parse.LetterOrDigit.Many()
    from trailing in Parse.WhiteSpace.Many()
    select new string(first.Concat(rest).ToArray());

var id = identifier.Parse(" abc123  ");

Assert.AreEqual("abc123", id);
```

# Проблемы и задачи парсинга

Далее я опишу несколько проблем и задач, с которыми мне пришлось столкнуться
на практике при использовании генератора парсера ANTLR версии 4 и полноценного
фреймворка Roslyn.

Несмотря на то, что это различные инструменты: Roslyn написан вручную,
парсит и анализирует C# и VB, ANTLR представляет собой генератор парсеров,
принимающий на вход грамматику и возвращающий парсер на целевом рантайме.
Сейчас поддерживается довольно много рантаймов: Java, C#, Python 2|3, JavaScript,
C++, Go, Swift. Причем под C# целых два рантайма от разных авторов: основной
и оптимизированный. Тем не менее, по некоторым параметрам Roslyn и ANTLR схожи,
и я попытался выполнить их сравнение.

В конце заголовка у некоторых задач в скобочках стоит название PT.PM, Roslyn или
Swiftify. Оно означает проект, в котором данная задача появилась и была решена.
PT.PM подразумевает грамматики SQL языков, PHP, Java, а также экспериментальную C#.

* Неоднозначность
* Регистронезависимость
* Островные языки и конструкции
* Парсинг фрагментов кода
* Скрытые токены
* Препроцессорные директивы
* Обработка и восстановление от ошибок
* Использование парсеров в IDE
* Производительность

# Неоднозначность

Обычно множество токенов подразделяют на литералы (строки, целые числа),
идентификаторы и ключевые слова. Ключевые слова используются при описании
синтаксических конструкций, а идентификаторы - для произвольных пользовательских
обозначений, например для названий классов, имен переменных и т.д. Литералы
никогда не пересекаются с ключевыми словами, чего не скажешь об идентификаторах.
Эта особенность хоть и усложняет парсинг, однако позволяет поддерживать
обратную совместимость в случае добавления новых слов и расширять множество
возможных идентификаторов.

Дальше речь идет о разрешении **неоднозначностей** на уровне токенов или
разграничении ключевых слов и идентификаторов.

На этом слайде показан пример: `var var = 100500;`

В ANTLR и других генераторах парсеров такая проблема решается добавлением
токена `VAR` в правило `identifier`, которое содержит в себе как обычный токен
`ID`, так и список ключевых слов, которые могут являться идентификаторами.
Далее это правило `identifier` используется во всех правилах грамматики вместо
обычного токена `ID`.

```ANTLR
// Lexer
VAR: 'var';
ID:  [0-9a-zA-Z];

// Parser
varDeclaration
    : VAR identifier ('=' expression)? ';'
    ;

identifier
    : ID
    // Other conflicted tokens
    | VAR;
```

# Объектный конструктор в C#

Roslyn способен разруливать и более сложные вещи, например, инициализацию
свойств в объектом конструкторе, при котором он понимает, что слева находится
свойство создаваемого объекта, а справа - сам объект. На этом слайде показан
пример.

# Какой результат возвращает `nameof`?

Также хочу упомянуть об еще одной особенности парсинга языковой конструкции `nameof`
в C# 6 и выше. Вернемся к классу `Foo` и его свойству `Bar`. Как думаете, что
вернет `nameof` в этом коде?

<!-- Ожидание реакции и ответов из зала -->

# `nameof` как функция и оператор

Ну на самом деле этот оператор может возвращать здесь все что угодно, например, `42`.
Дело в том, что одним из требований новых версий C# является обратная совместимость.
А такой метод в теории где-то может быть определен. Поэтому он и используется.
А иначе все методы `nameof` в старом коде перестали бы работать. То же касается
слов `async`, `await` и некоторых других.

<!-- TODO: 👨‍💻 Возможно добавить еще интересных примеров -->

# Неоднозначность: решение с использованием вставок кода

Возвращаясь к генератору парсеров ANTLR, проблему с отделением ключевых
слов от идентификаторов можно решить и с помощью вставок кода на целевом
языке, которые называются **действиями** (**action**) или **семантическими предиктами**. Они
используются в случае, если мощности контекстно-свободной грамматики не достаточно
для парсинга каких-то конструкций или требуется выполнять какие-то инструкции во
время парсинга. Обособляются они фигурными скобками. Разница между действиями и
предикатами заключается в том, что последние должны возвращать какой-то результат
и для них используется вопросительный знак в конце. Вот так выглядит простейший
пример использования предикатов:

```ANTLR
// Lexer
ID:  [0-9a-zA-Z];

// Parser
varDeclaration
    : id {_input.Lt(-1).Text == "var"}? id ('=' expression)? ';'
    ;

id
    : ID;
```

Как видим, в загадочной конструкции `_input.Lt(-1)` проверяется текст последнего
токена и, если он равен "VAR", то для парсинга декларации используется
соответствущая альтернатива. Таким образом можно считать некоторые слова
ключевыми но только во время парсинга, а не на этапе токенизации.

# Контекстно-зависимые конструкции (PT.PM)

Всю логику, которая не касается парсинга, рекомендуется прописывать в
специальных классах, которые реализуют шаблоны проектирования **Visitor** или
**Walker**, по-другому **Listener**. Однако бывают ситуации, когда без вставок
кода просто никак не обойтись. Например, при парсинге конструкций
[Heredoc](http://php.net/manual/en/language.types.string.php) в PHP.

Данная конструкция определяет многострочную строку. Она интересна тем, что
начинается со специального маркера и должна заканчиваться точно таким же маркером.
Формально говоря, парсинг зависит не только от типа токена, но и от его значения.
В примере на данном слайде таким маркером является `HeredocIdentifier`.

```PHP
<?php
    echo <<< HeredocIdentifier
Line 1.
Line 2.
HeredocIdentifier
;
```

В данном случае невозможно написать грамматику так, чтобы она без вставок
кода это парсила. Решение со вставками кода можно посмотреть в официальном репозитории
грамматик [PHP](https://github.com/antlr/grammars-v4/blob/master/php/PHPLexer.g4).

К сожалению, Antlr в настоящее время не поддерживает
[универсальные вставки кода](https://github.com/antlr/antlr4/issues/1045), т.е.
вставки кода которые транслировались бы на целевой язык рантайма. Из-за этого
приходится дублировать такие грамматики под разные рантаймы, что выглядит
некрасиво. [JavaScript](https://github.com/antlr/grammars-v4/tree/master/ecmascript),
например, является таким примером.

Вы можете задаться вопросом: а зачем вообще разработчики PHP все так усложнили?
Дело в том, что удобно, когда строки выглядят чисто и не содержат захламляющие символы
экранирования. Однако это получается не всегда и по крайней мере надо как-то
обозначать маркеры начала и окончания строк. Крайне маловероятно, что в Heredoc
попадется строка с `HeredocIdentifier`, однако если попадется, то можно просто
использовать другой идентификатор.

# Интерполируемые строки в C# 6

Но PHP является не единственным примером. Начиная с 6 версии в C# появилась
**интерполируемых строк**. Как вы уже наверное знаете, в строке может находиться не только
само выражение, но и описание формата вывода (выравнивание, количество нулей
после запятой и т.д.). И парсить такие строки довольно сложно, потому что, например,
закрывающаяся фигурная скобка может означать как часть интерполируемого выражения,
так и выход из самого режима интерполяции. А двоеточие
может также быть как частью выражения так и означать конец выражения и описание
формата вывода (например, #0.##). Кроме того, такие строки могут быть как
обычными (*regular*), так и без экранирующих символов (*verbatium*), а также могут
быть вложенными друг в друга. Короче все сложно с ними! Подробнее синтаксис
описан на [MSDN](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/interpolated-strings).

```CSharp
s = $"{p.Name} is \"{p.Age} year{(p.Age == 1 ? "" : "s")} old";
s = $"{(p.Age == 2 ? $"{new Person { } }" : "")}";
s = $@"\{p.Name}
                       ""\";
s = $"Color [ R={func(b: 3):#0.##}, G={G:#0.##}, B={B:#0.##}, A={A:#0.##} ]";
```

В C# Antlr парсере интерполяция строк была реализована с использованием стека для
подсчета уровня текущей интерполируемой строки, а также фигурных скобок. Все это
реализовано в
[CSharpLexer.g4](https://github.com/antlr/grammars-v4/blob/master/csharp/CSharpLexer.g4).

# Лексерный хак: плохое решение

В теории парсинга есть интересная [задачка](https://en.wikipedia.org/wiki/The_lexer_hack).

<!-- TODO: доделать -->

`specifier` описывает тип и может являться как ключевым словом, например `static`,
`int`, так и произвольным типом, например `MyType` как на этом слайде. Правило
`initDeclaratorList` также может являться идентификатором. Однако как минимум
одна инициализация должна присутствовать, но парсер работает жадно и распознает
при такой грамматике все токены как специфиры, поэтому в визиторе приходится
переделывать последний тип, а именно `staticGlobalVar` в переменную. Это
выглядит громоздко и некрасиво. Ситуацию еще осложняет то, что в Objective-C
поддерживается парсинг объявлений переменных без самих идентификаторов переменных,
например [`unsinged int;`](https://goo.gl/kKhdCh). Т.е. просто так знак вопроса
из грамматики не убрать. Однако данное правило все же можно переписать в
одну строчку таким образом, чтобы парсер справлялся со всеми этими проблемами.
Кто-нибудь выскажет предположение как?

<!-- Ожидание ответов из зала -->

# Лексерный хак: изящное решение (Swiftify)

Вот это решение.

Благодаря тому, что ANTLR обладает мощным алгоритмом парсинга LL(*), который
способен анализировать произвольное количество токенов для корректного
распознавания, правило выше для парсинга деклараций можно переписать по-другому:

```ANTLR
varDeclaration
    : (specifiers initDeclaratorList | specifiers) ';'
    ;
```

Интересно, что запись из прошлого слайда и из этого не эквивалетны, несмотря
на то, что оператор ? раскрывается по сути в такую же конструкцию.
Первое правило всегда будет в приоритете, а второе будет использоваться в
случае, если распарсить первой альтернативой никак не удалось.

# Регистронезависимость

Некоторые языки программирования являются нечувствительными к регистру.
Это в основном характерно для языков, изначально предназначенных для не
технических пользователей и учебных (Delphi). SQL языки вообще изначально
разрабатывались для аналитиков и людей, напрямую не связанных с программированием.
Однако хотели как лучше, а получилось как всегда: сейчас ими и ORM библиотеками
только программисты и пользуются.

Дело в том, что простым пользователям не очевидно, отличается ли имя с большой
буквы от имени с маленькой, т.к. в обычных неформальных переписках это не влияет
на понимание текста.

В ANTLR нечувствительность к регистру входного потока можно достигнуть двумя
способами: либо использовать **фрагментные токены** или же использовать средства
рантайма.

На данном слайде описан первый способ. Фрагментные токены отличаются от обычных
тем, что они служат кирпичиками, из которых составляются реальные токены. А при
токенизации они никак не учитываются. Т.е. они используются только для упрощения записи.

```ANTLR
Abstract:           A B S T R A C T;
BoolType:           B O O L E A N | B O O L;
BooleanConstant:    T R U E | F A L S E;

fragment A: [aA];
fragment B: [bB];
```

Без *фрагментных токенов* грамматика бы выглядела более громоздко как видно
внизу.

```ANTLR
Abstract:           [Aa][Bb][Ss][Tt][Rr][Aa][Cc][Tt];
```

Видите, какая некрасивая запись. И это только один токен!

# Регистронезависимость: решение на уровне рантайма (PT.PM)

Также можно нормализовать регистр входного потока символов и далее уже использовать
стандартные токены. Недостатком является то, что информация о регистре полностью
удаляется, а она может быть иногда важна. Например, в том же PHP ключевые слова
являются нечувствительными к регистру, а названия переменных - чувствительными.
Например, `$id` и `$ID` - разные переменные. К счастью, ANTLR поддерживает
специальный [поток](https://gist.github.com/sharwell/9424666)
сохраняющий регистр у значений токенов, но при этом не учитывающий его при
лексическом анализе. Код лексера становится еще чище, а скорость парсера - еще выше.
С помощью такого подхода сейчас и написаны грамматики SQL в официальном репозитории.

# Островные языки и конструкции (PT.PM)

Существуют языки, фрагменты которых вложены в другие языки. Такие языки называются
**островными**, а типичными примерами являются JavaScript внутри PHP или,
например, вставки C# кода внутри Aspx.

Может возникнуть вопрос: Зачем эти языки парсить вместе? Ведь сначала на сервере
отрабатывает парсер PHP, а затем на клиенте в браузере парсится JavaScript без
единого PHP блока. Однако иногда удобно парсить все в одном месте, например, в
статических анализаторах кода.

```PHP
<?php
<head>
  <script type="text/javascript">
    document.body.innerHTML="<svg/onload=alert(1)>"
  </script>
</head>
```

При парсинге таких языков требуется использовать как разные типы токенов,
так и разные типы синтаксических структур.

К счастью, в ANTLR и других генераторах парсеров существует возможность
переключения режимов распознавания лексем по определенным последовательностям
символов или условиям. В примере на слайде такими маркерами являются открывающий и
закрывающий теги `<script>` с или без атрибутов.

После этого для полученного потока токенов последовательно используется два
парсера. Сначала парсится код на PHP, при этом вставки JS трактуются как строки,
а затем, на этапе обхода дерева разбора PHP, парсятся вставки JS кода, причем
к каждому обработанному узлу добавляется смещение относительно текущего
фрагмента кода.

Кстати, усилиями Positive Technologies была взята из Mono и
доработана библиотека [AspxParser](https://github.com/PositiveTechnologies/aspxparser)
для парсинга Aspx файлов. Она распространяемая по лицензии MIT.

# PHP: альтернативный синтаксис (PT.PM)

Вот еще пример кода на PHP, в котором используется так называемый
[**альтернативный синтаксис**](http://php.net/manual/ru/control-structures.alternative-syntax.php)
и блоки HTML перемешиваются с блоками PHP кода. Если кто не понял, то здесь
"запрятана" конструкция `switch case`.

```PHP
<?php switch($a): case 1: // without semicolon?>
        <br>
    <?php break ?>
    <?php case 2: ?>
        <br>
    <?php break;?>
    <?php case 3: ?>
        <br>
    <?php break;?>
<?php endswitch; ?>
```

# Использование отдельного режима лексем для JavaScript (PT.PM)

В ANTLR переключение режима лексем осуществляется с помощью команд
`pushMode`, `mode`, `popMode` и выглядит это примерно так:

```ANTLR
ScriptText:   ~[<]+;
ScriptClose:  '</script'? '>' -> popMode;
PHPStart:     '<?=' -> type(Echo), pushMode(PHP);
PHPStart:     '<?' 'php'? -> channel(SkipChannel), pushMode(PHP);
ScriptText2:  '<' ~[<?/]* -> type(ScriptText);
```

В этом фрагменте лексера также используются команды `type`, которая изменяет
тип токена и `channel`, которая помещает токен в отдельный изолированный канал.
Эту команду можно использовать и для изоляции комментариев и пробелов.

# Обработка комментариев и пробелов

Кстати, о них. На самом деле обработка комментариев, пробелов и других
токенов, которые не играют никакой значимой роли для компилятора,
не такая уж простая задача, как может показаться на первый взгляд. Их можно
просто удалять или помещать в скрытый поток, тогда они не будут мешаться при
парсинге. Однако в этом случае теряется их связанность с деревом разбора,
что может быть критично при трансформациях дерева, например, при рефакторинге. Если
говорить про грамматики, то можно попытаться связывать такие токены с основными
включив их непосредственно во все правила примерно как в первом пункте:

```ANTLR
declaration:
    property COMMENT* COLON COMMENT* expr COMMENT* prio?;
```

Понятное дело, что такое решение никуда не годится. Грамматика станет
громоздкой и избыточной, т.к. повсюду будут мельтешить комментарии и пробелы.

В связи с этим было проанализировано два подхода по связыванию скрытых токенов
с узлами дерева разбора:

* Первый заключается в связывании скрытых токенов с нетерминальными узлами
  дерева разбора.
* А второй - в связывании с терминальными узлами или токенами.

# Связывание скрытых токенов с узлами дерева (Swiftify)

В нашем сервисе Swiftify используется пока что первый способ.В этом случае
множество токенов разбивается на предшествующие, последующие и сироты. Ранее
была найдена и доработана реализация для ANTLR 3. 

Рассмотрим пример с двумя выражениями присваивания и фигурными скобками.
**Предшествующие** (Precending) токенами располагаются перед узлами присваивания.
В этом множество также попадает и начальный комментарий в документе (`//First Comment`).

```С
//First comment
'{' /*Precending1*/ a = b; /*Precending2*/ b = c; '}'
```

В случае, если после токена существует только терминальный символ, он заносится
во множество **последующих** токенов (Following). Последний комментарий также
попадает в это множество.

```С
'{' a = b; b = c; /*Following*/ '}' /*Last comment*/
```

Все остальные скрытые токены, которые располагаются между двумя значимыми токенами,
попадают в список **сирот** (Orphans).

```С
'{' /*Orphan*/ '}'
```

Более подробное о таком связывании написано в статье
[Tackling Comments in ANTLR Compiler](http://meri-stuff.blogspot.ru/2012/09/tackling-comments-in-antlr-compiler.html).
В данном способе есть недостаток: скрытые могут быть связаны со всеми типами
нетерминальных узлов, и обрабатывать их не очень удобно.

# Связывание скрытых токенов со значимыми (Roslyn)

Другой подход заключается в связывании скрытых токенов с терминальными узлами
дерева разбора или значимыми токенами. Такой подход реализован в Roslyn.
Сначала я опишу типы узлов синтаксического дерева Roslyn:

**Node** - по сути является нетерминальным (т.е. не конечный) узлом, который хранит в себе
несколько других узлов и отображаюет определенную конструкцию.

**Token** - это терминальный узел дерева, значимый токен. Представляет ключевое
слово, идентификатор, литерал или пунктуацию. Например, число `42` или `;`.

А вот **Trivia** - более интересный токен, представляющий комментарии, пробелы
и другие скрытые токены. Он не является узлом дерева, однако связывается с его
токенами. Такие узлы оказываются незаменимыми при трансформации дерева обратно в код
(например, при рефакторинге). Сами тривии могут быть **лидирующими** (*Leading*) и
**замыкающими** (*Trailing*).

Во множество замыкающих попадают все тривии на той же самой строчке от
значимого токена до следующего значимого токена. В примере это комментарии
traiking. Все остальные скрытые токены после рассматриваемого попадают во
множество лидирующих и связываются со следующим значимым токеном. В примеры
это комментарии leading. Первый значимый токен содержит в себе начальные
тривии файла. Скрытые токены, замыкающие файл, связываются с последним
специальным end-of-file токеном нулевой длины.

```CSharp
// leading 1 (var)
// leading 2 (var)
var foo = 42; /* trailing (;)*/ int bar = 100500; // trailing (;)

// leading (EOF)
EOF
```

Такое связывание позволяет лучше формализовать положение скрытых токенов и даже
отключенного кода из препроцессорных директив!

# Препроцессорные директивы (Roslyn)

Напомнию, что *препроцессорными директивами* назваются вставки на специальном
языке, с помощью которого можно отключать куски кода при определенных условиях.
Их часто используют для разных методов API в разных версиях .NET фреймворков.

В коде на слайде при заданном символе `NETCORE` отключенный (задизабленный) код
попадет в список *Leading* для точки с запятой `;`, а текст `#if NETCORE` - в
список *Leading* для токена `true`.

```CSharp
bool trueFlag =
#if NETCORE
    true
#else
    new Random().Next(100) > 95 ? true : false
#endif
;
```

<!-- TODO: проверить на всякий случай -->

# Препроцессорные директивы: одноэтапная обработка (Swiftify)

В ANTLR токены директив заносятся в специальный канал и при обходе дерева
разбора они парсятся. Это похоже на обработку островных языков. Такой подход
позволяет интерпретировать и обрабатывать макросы как обыкновенные функции
(при удалении директив макросы бы инлайнились, что нам не очень подходило):

```Objective-C
#define DEGREES_TO_RADIANS(degrees) (M_PI * (degrees) / 180)
```

```Swift
func DEGREES_TO_RADIANS(degrees: Double)
    -> Double { return (.pi * degrees)/180; }
```

Потестить обработку директив уже можно сейчас в ([Swiftify](http://objc.to/l4bamg)).
Однако при таком способе обработки возможны потенциальные ошибки парсинга для
незаконченных фрагментов кода.

<!-- TODO: добавить незаконченный фрагмент кода -->

# Препроцессорные директивы: двухэтапная обработка (Codebeat)

Если говорить о стандартном способе обработки препроцессорных директив, то
сначала обрабатывается код директив, а затем уже используется обычный парсер.
Например, для кода выше входной код для парсера будет выглядеть следующим образом.

```CSharp
bool·trueFlag·=
·········
····true
·····
··············································
······
;
```

Обратите внимание, что пробелы (ну и проблемы 🙂) сохраняются, т.к. они
нужны для корректного отображения ошибок и локации (TextSpan) у всех реальных узлов.

Последовательность действий при этом способе такая:

Сначала токенизируются и разбираются диреткивы.

Затем вычисляются условные директив `#if` и определяются компилируемые блоки кода.

После этого директивы заменяются на пробелы.

А в конце токенизируется и парсится результирующий текст с удаленными директивами.

Такой способ двухэтапной обработки также реализован для грамматики Objective-C
и используется в веб-сервисе для подсчета метрик кода и проведения ревью
[Codebeat](https://codebeat.co/).

# Парсинг фрагментов кода (Swiftify, PT.PM)

Входной единицей компилятора обычно является целый файл, т.е. если говорить
про C#, то такой файл в начале содержит список `using`-гов, затем идет объявление
`namespace`, декларация класса, ну и т.д. Т.е. известно с чего парсинг начинается.
Если говорить в терминах ANTLR и грамматик, то известно корневое правило парсинга,
обычно это что-то вроде `topDeclaration` или `compilationUnit`.

Однако что если нужно правильно распарсить фрагмент кода? Такое может использоваться
в IDE при рефакторинги или, например, для конвертинга кусочка кода.
Задача усложняется если в этом фрагменте еще есть и ошибки.

Эту задачу можно решить разными способами: с использованием операций на строках и
регулярных выражений, с помощью токенов и полноценного парсинга. Можно пытаться
определить корневое правило по ключевым словам, однако они могут встретиться и
в комментариях. С токенами тоже не все так гладко. В итоге было разработано
решение, которое заключается в парсинге фрагмента разными правилами и выборе правила
без ошибок или правила с максимально близкой к концу позицией. Пробовали выбирать
правила с минимальным количеством ошибок, но это показало худшие результаты.

По ссылкам доступны примеры таких фрагментов, ну вы и сами можете попробовать
ввести свои и потестить если знаете Objective-C.

* [Утверждения](http://objc.to/6mpmhq)
* [Декларации методов](http://objc.to/nt25a1)
* [Свойства](http://objc.to/vnpasw)

# Ошибки парсинга

Кстати, об ошибках. Важной способностью каждого парсера является обработка ошибок,
т.к. парсер не должен падать только из-за одной ошибки, а должен "понимать",
что это за ошибка и пытаться парсить код дальше. Он должен возвращать
минимальное количество ошибок и отображать наиболее понятное описание.
Тут можно вспомнить про C++ с кучей ошибок в случае неправильно написанного
шаблона.

У Roslyn и ANTLR коды ошибок в чем-то похожи, но последний конечно хуже
с ними справляется из-за своей универсальности.

В случае, если лексер ANTLR не может подобрать лексическое правило для входного
символа, то генерируется **ошибка распознавания токена**. Плохо то, что
в случае кракозябр будет тьма ошибок, равная количеству этих кракозябр. В
Roslyn такого не будет. Однако с помощью специального правила
`ERROR: . -> channel(ErrorChannel)` можно помещать все неправильные символы в
отдельный канал.

Вверху пример кода с лексической ошибкой. Не распознана решетка на месте идентификатора
класса.

```CSharp
class # { int i; }
```

Все остальные ошибки касаются парсера. Часто возникают ситуации, в которых
разработчик не дописал нужный символ или наоборот, написал лишний.
В следующем примере вверху пропущена скобка, а внизу добавлена лишняя точка с запятой
после идентификатора класса. Такие ошибки назвают ошибками с единичным
отсутствующим и лишним токенами.

```CSharp
class T { int f(x) { a = 3; } // Отсутствующий токен
class T ; { int i; } // Лишний токен
```

В случае если лишних токенов несколько, то в парсере активируется режим
"паники", и он пытается пропустить лишние токены, пока не встретит конечный,
синхронизирующий. Таким может являться, например, `;` или `}`. Вот простеший
пример:

```CSharp
class T { int f(x) { a = 3 4 5; } }
```

Все остальные ошибки классифицируются как отсутствующее альтернативное
подправило:

```CSharp
class T { int ; }
```

# Ошибки парсинга в Roslyn

На этом слайде показан пример C# кода, в котором удалось воспроизвести
некоторые синтаксические ошибки Roslyn.

Первой является **лишний синтаксис** (*Skipped Trivia). Это случайно напечатанный
символ или `;` как здесь. При этой ошибке Roslyn создает отдельный узел с
`Kind = SkippedTokensTrivia`.

Вторая ошибка - **недостающий синтаксис**. В этом случае Roslyn достраивает
соответствующий узел со значением свойства `IsMissing = true`. Типичный пример -
Statement без точки с запятой.

Для следующих ошибок **некорректное значение численного, строкового или символьного литерала**
создается отдельный узел с Kind, равным `NumericLiteralToken`,
`StringLiteralToken` или `CharacterLiteralToken`. Это ошибка, например,
слишком большого значение либо пустого char.

Последняя ошибка - **незавершенный член**. При этом создается отдельный узел
`IncompleteMember`;

Как видим, ошибки *недостающий синтаксис* (Missing) и *лишний синтаксис*
(Skipped Trivia) похожи на аналогичные ошибки в ANTLR.

```CSharp
namespace App
{
    class Program
    {
        ;                    // Skipped Trivia
        static void Main(string[] args)
        {
            a                // Missing ';'
            ulong ul = 1lu;  // Incorrect Numeric
            string s = """;  // Incorrect String
            char c = '';     // Incorrect Char
        }
    }

    class bControl flow
    {
        c                    // Incomplete Member
    }
}
```

Как уже говорилось в начале, парсеры, написанные вручную, более гибкие. Это
касается в том числе и восстановления от ошибок. Видите, здесь даже подсветка
поплыла, а Roslyn выдал всего 6 ошибок!

Благодаря таким продуманным типам синтаксических ошибок и связыванию скрытых
токенов, Roslyn может преобразовать дерево с любым количеством ошибок обратно в
код символ в символ. Такое дерево называют **достоверным** (или **full fidelity**).

# Уязвимость goto fail

К сожалению, последняя версия ANTLR 4.7 пока что не позволяет строить достоверное
дерево без доработки напильником. Такое дерево очень необходимо в некоторых задачах.
Вы наверное знакомы с нашумевшим багом в SSL [goto fail](https://nakedsecurity.sophos.com/2014/02/24/anatomy-of-a-goto-fail-apples-ssl-bug-explained-plus-an-unofficial-patch/),
который заключался в том, что разработчики не убрали лишнюю строчку
из-за которой в алгоритме проверки подлинности пропускались какие-то шаги.

В контексте данного доклада не так важно в чем именно была уязвимость, однако
интересно то, что ее можно обнаружить как с помощью анализа достоверного дерева разбора,
так и с помощью анализа **графа потока управления** (*CFG*).

Первым способом можно выяснить, что с дублирующемся утверждением `goto fail`
что-то не так: он вроде бы написан в родительском блоке, но при этом почему-то
сдвинут, в отличие от остальных утверждений, например `if`-ов. Здесь по-любому
что-то не так и анализаторам следует выводить предупреждение!

```C
hashOut.data = hashes + SSL_MD5_DIGEST_LEN;
hashOut.length = SSL_SHA1_DIGEST_LEN;
if ((err = SSLFreeBuffer(&hashCtx)) != 0)
    goto fail;
// ...
if ((err = SSLHashSHA1.update(&hashCtx, &signedParams)) != 0)
    goto fail;
    goto fail;  /* MISTAKE! THIS LINE SHOULD NOT BE HERE */
```

А с помощью анализе графа потока управления можно выяснить, что после `goto fail` идут
какие-то инструкции, которые никогда не выполнятся, ну т.е. на лицо недостижимый код.
Однако анализ таких графов более трудоемок и сложен. Более того, чем больше
уровней анализа, тем более полную картину можно составить о качестве кода .

# Заключение о парсинге

Я затронул не все темы парсинга, не освятить тему использования в IDE,
автокомплита, форматирования кода (эдакого антипарсинга), оптимизации.
Но так как у нас ограниченное время, то хочу еще немного освятить тему обработки
древовидных структур в .NET.

# Обработка древовидных структур

Как я уже говорил, парсер возвращает древовидную структуру, которая называется
*деревом разбора*.

Деревья нужно как-то обрабатывать. Сложность представляет сообой то, что такие
структуры рекурсивные и иерархические. И для их обхода используются специальные шаблоны
проектирования, которые называются *Visitor* и *Listener*. Эти шаблоны можно
проектировать и реализовывать по-разному. Помимо обхода, деревья еще можно
сериализовывать, т.е. сохранять их состояние для последующего использования или
передачи. Также при различных операциях с деревом нередко всплывают вопросы
производительности.

# Методы обхода деревьев

В Visitor для обработки потомков какого-либо узла необходимо вручную вызывать
методы их обхода. При этом если родитель имеет три потомка, и вызвать методы
только для двух узлов, то часть поддерева вообще не будет обработана.
В Listener (Walker) же методы посещения всех потомков вызываются автоматически.
В Listener существует метод, вызываемый в начале посещения узла (`EnterNode`) и
метод, вызываемый после посещения узла (`ExitNode`). Эти методы также можно
реализовать с помощью механизма событий. Методы Visitor, в отличие от Listener,
могут возвращать объекты и даже могут быть типизированными, т.е. при объявлении
`CSharpSyntaxVisitor` каждый метод Visit, будет возвращать объект AstNode, который
в нашем случае является общим предком для всех остальных узлов AST.

Таким образом, при использовании паттерна проектирования Visitor, код
преобразования дерева получается более функциональным и лаконичным за счет того,
что в нем нет необходимости хранить информацию о посещенных узлах. На рисунке
ниже можно увидеть, что, например, при преобразовании языка PHP, ненужные узлы
HTML и CSS отсекаются. Порядок обхода обозначен числами. Listener обычно
используется при агрегации данных (например из файлов типа CSV). В принципе
любой Listener является частным случаем визитора, поэтому на нем я далее
останавливаться не буду.

![Visitor & Listener](https://habrastorage.org/files/bd1/69c/535/bd169c535e854f9681520f520d0db9c3.png)

# Реализации Visitor

Посетители и слушатели могут быть написаны вручную, сгенерированы, а также
разработаны с использованием рефлексии и DLR, т.е. ключевого слова `dynamic`.

Код, написанный вручную, опять-таки предоставляет максимальную гибкость и
производительность. Однако размер исходника напрямую зависит от количества
узлов в обрабатываемом дереве, ну и такие визиторы долго и утомительно разрабатывать.
А чем больше код, тем легче в нем ошибиться. Пример визитора с кодом, написанным
вручную я показывать не буду, потому что он достаточно тривиальный.

Также визиторы могут быть очень быстро сгенерированы. В ANTLR, например, для каждого
правила грамматики генерирется отдельный метод Visit. Однако не всегда есть
возможность генерации. Другим недостатком является то, что код внутренностей
визиторов хуже воспринимается, избыточный, и может нарушать какой-то
проектный Code Style. Например, если слова в правилах разделяются символами
подчеркивания, то в названиях методов Visit также будут некрасивые подчеркивания.

# Диспетчеризация с использованием рефлексии и dynamic

Разумным компромиссом является третий способ, а именно, визитор с использованием
рефлексии и ключевого слова `dynamic`. Обход деревьев и других графовых структур,
кстати, является наглядной демонстрацией того, где можно применять это слово, если до
этого вы его нигде не исползовали.

Суть данного способа заключается в том, что логика обхода узлов по-умолчанию
прописывается не в каждом методе `Visit`, а в одном методе обхода узлов
по-умолчанию  `VisitChildren`, который извлекает свойства для каждой ноды и
производит их обход. Интересным моментом является и динамическая диспетчеризация
с использованием `dynamic`. Например, в этом коде DLR определит метод `Visit`,
который нужно посетить для объекта `customNode`: `ExpressionStatement`,
`StringLiteral` или каким-то другим аргументом.

```CSharp
// invocation in VisitChildren
Visit((dynamic)customNode);

// visit methods

public virtual T Visit(ExpressionStatement expressionStatement)
{
    return VisitChildren(expressionStatement);
}

public virtual T Visit(StringLiteral stringLiteral)
{
    return VisitChildren(stringLiteral);
}
```

Такая реализация может иметь какие-то проблемы с производительностью, однако
хорошо подходит для прототипов. Мы в Positive Technologies, например, сейчас
используем такие визиторы при разработке универсального анализатора кода на
основе потока данных или Taint движке. Простой пример реализации можно найти
в проекте [TreeProcessing.NET](https://github.com/KvanTTT/TreeProcessing.NET).

# Архитектура Visitor

Последняя вещь, которую я хотел бы упомянуть о визиторе - это их реализация.
Можно использовать несколько классов-визиторов, которые будут отвечать за обработку
своих узлов, например, `Declaration`, `Expression`, `Statmement` и другие.
А можно использовать один большой визитор, в котором будут прописаны визиторы
для всех узлов.

В Swiftify мы сначала использовали первый подход, но потом перешли на второй, т.к.
это позволило упростить код и избавиться от лишних аллокаций ненужных визиторов.
К счастью, C# поддерживает частичные классы, что позволило разбить реализацию
такого гиганского визитора на несколько файлов. Однако и первый подход имеет
свои преимущества по сравнению с единым: для каждого визитора можно определить
свой возвращаемый тип. Например, визиторы для выражений могут возвращать какие-то
выражения, визитор для методов и классов может возвращать их декларации.
Однако для нас это оказалось не принципиальным, поскольку при конвертинге
кода на выходе всегда получается строка.

Более того такой визитор позволил лучше формализовать процесс обхода дерева,
т.к. в нем переопределяются все методы и для узлов, которые точно не должны
посещаться, используется заглушка `throw new ShouldNotBeVisitedException(context);`.

# C\# 7: локальные функции

Как известно, последней версией C# является 7. В нем появилось достаточно
много нововведений, таких как out переменные, Pattern Matching в том числе включая is,
кортежи, локальные функции, улучшения записи литералов и другие фичи.

Некоторые фичи очень востребованы и они облегчают запись кода, а применение
других не совсем очевидно, например, *локальные функции*. На мой взгляд, такие
функции может и не так сильно сокращат запись, зато хорошо организуют рекурсию.
У рекурсии есть инициализация и основное действие. На этом слайде показан
код метода, возвращающий список терминальных узлов (листьев) для переданного узла.
При инициализации создается список, а при обходе он заполняется. Без локальных
функций пришлось бы писать вторую функцию, которая использовась бы только в этом
месте.

```CSharp
public static List<Terminal> GetLeafs(this Rule node)
{
    var result = new List<Terminal>();
    GetLeafs(node, result);

    // Local function
    void GetLeafs(Rule localNode, List<Terminal> localResult)
    {
        for (int i = 0; i < localNode.ChildCount; ++i)
        {
            IParseTree child = localNode.GetChild(i);
            // Is expression
            if (child is TerminalNode typedChild)
                localResult.Add(typedChild);
            GetLeafs(child, localResult);
        }
    }
    return result;
}
```

Конечно в C# давно уже есть и лямбда функции, однако они вызываются через
делегат, а значит менее быстрые. Также они не могут быть рекурсивными,
generic и в конце концов код с ними хуже выглядят.

<!-- TODO: ⌨ Пример с двумя функциями, локальной и делегатами -->

# Сериализация

Форматы сериализации бывают следующими:

* Бинарная
* XML, JSON, YAML и другие языки разметки
* Собственный формат

Как известно, бинарная сериализация очень быстрая, но формат нечитаем для человека.
XML и Json форматы наоборот, удобны для чтения и редактирования, однако отстают
по скорости от бинарной. Под .NET существуте большое количество библиотек
сериализации, такие как Protobuf, MessagePack, Json.NET, а также встроенные
во фреймворк. Подробно я на них останалвиваться не буду, однако поделюсь ссылкой
на недописанную с 2015 статью, в которой подробно рассматривается сериализация
древовидных структур и другие вещи: [Сериализация, отображение, сравнение и обход древовидных структур в .NET](https://github.com/KvanTTT/Articles/blob/tree-structures-serialization-comparison-and-mapping-on-net/Tree-Structures-Serialization-Comparison-and-Mapping-on-NET/Russian.md).
А реализация находится в вышеупомянутом проекте [TreeProcessing.NET](https://github.com/KvanTTT/TreeProcessing.NET).

К сожалению, не все библиотеки умеют работать с деревьями из коробки и иногда
требуется доработка напильником, чтобы сериализация вообще работала, результат
был независим от рантайма и был максимально приятен для человека.

# JSON сериалзиация деревьев

Я расскажу о том, как мы приручили популярную библиотеку Json.NET для такого
вывода. Дело в том, что если свойство имеет абстратный класс, который
конкретизируется во время исполнения, но, понятное дело, необходимо записывать
информацию об этом конкретном типе для последующей корректной десериализации.
Например, свойство `Expression` может конкретизироваться `BinaryOperatorExpression`
или `StringLiteral`. Json.NET поддерживает из коробки параметр `TypeNameHandling.All`
для обработки типов, однако результирующий JSON выглядит многословным и зависимым
от рантайма (т.е. `System.Collections.Generic.List` - дотнетовская сущность).

```JSON
"Statements": {
    "$type": "System.Collections.Generic.List`1[[TreesProcessing.NET.Statement, TreesProcessing.NET]], mscorlib",
    "$values": [
        ...
    ]
```

# Своя логика сериализации с JsonConverter

К счастью, Json.NET поддериживает класс, позволяющий переопределить
логику сериализации и десериализации определенных узлов, `JsonConverter`.

В качестве идентификатора типа конкретного класса можно использовать как само
имя класса, так и самостоятельно определенное свойство или аттрибут.
Дополнительным плюсом Enum свойства является то, что его можно использовать не
только при сериализации, но и для более быстрого сравнения двух узлов без
использования операторов приведения, что также можно использовать.

* **Имя класса** в качестве идентификации ьтипа
  ```CSharp
  BinaryOperatorExpression
  ```
* **Свойство** в качестве идентификации типа
  ```CSharp
  override NodeType NodeType => NodeType.InvocationExpression;
  ```
* **Аттрибут** в качестве идентификации типа
  ```CSharp
  [NodeAttr(NodeType.BinaryOperatorExpression)]
  public class BinaryOperatorExpression : Expression
  ```

При десериализации используется `Activator`, который создает существующий
в сборке класс по переданной строке.

Результирующий JSON при таком подходе выглядит покороче:

```JSON
"Statements": {
    "NodeType": "BlockStatement",
    "Statements": [
        ...
    ]
```

# Оптимизация (Swiftify)

При обходе деревьев часто приходится обращаться к одним узлам по несколько раз.
Хорошо, если эти узлы - ближайшие, т.е. дети или родители. Однако если узел
расположен через несколько поколений, да еще и ищется по определенному условию,
то большое количество обращений к ниму может повлечь проблемы с производительностью,
особенно если файл большой. Для решения этих проблем можно кэшировать
информацию о посещенных ранее узлах для быстрого извлечения в дальнейшим. Такой
примем называется **мемоизацией**.

В Swiftify у нас есть метод, который возвращает первого потомка определенного
типа `FirstDescendantOfType`. После записи списка наследников для каждого
узла удалось добиться увеличения производительности более чем в два раза для
большого файла. Однако ничего не бывает бесплатным: потребление памяти
возросло примерно в 3 раза. В нашем случае потребление памяти все равно
было небольшим, так что это оказалось предпочтительней.

Также целесообразно избавляться от аллокаций объектов в методах, которые
часто вызываются. У нас таких методом является `Visit`: он вызывается
практически для каждого узла дерева разбора, т.к. содержит в себе логику
обработки скрытых токенов, а также обработчик исключений. Раньше там всегда
создавался `StringBuilder`, даже если child был один. Ну т.е. понятно, что можно
сразу его результат и возвращать. Данная оптимизация тоже немного уменьшила
потрбление памяти и ускорила скорость.

# Ресурсы

Я заканчиваю доклад. На этом слайде предоставлен список ресурсов, на основе
которых готовился материал. Как я уже говорил, практически все можно найти
на гитхабе под пермиссивными лиценизями, а сама презентация также доступна там.

Вообще рекомендую писать презентации в markdown формате - это очень удобно.

# Вопросы?

Теперь я готов ответить на ваши вопросы.
<!-- Ответы на вопросы-->